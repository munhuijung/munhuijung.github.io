```
#메인함수     
def my_data():
    print('='*15,'데이터 분석을 정말 편하게','='*15)
    print('1. 중앙일보 기사 데이터 스크롤링 \n2. 네이버 블로그 데이터 스크롤링 \n3. 긍정, 부정 감성분석 \n4. 워드클라우드 그리기 \n5. 유튜브 스크롤링 \n6. 구글 이미지 스크롤 \n7. 빙 이미지 스크롤 \n8. mySQL 과 연동하기 \n9. Oracle 과 연동하기')
    print('='*53)

    num = int(input('▶ 번호를 선택하세요: '))
    
    if num == 1:
        str1 = input('▶ 키워드를 입력하세요: ')
        num1 = int(input('▶ 더보기를 몇 번 누를까요?'))
        
        choongang(str1, num1)

    elif num == 2:
        str2 = input('▶ 키워드를 입력하세요: ')
        num2 = int(input('▶ 더보기를 몇 번 누를까요?', '한 페이지에 7개의 블로그가 있습니다.'))
        
        naver_blog(str2, num2)

    elif num == 3:    
        str3 = input('▶ 파일명을 입력하세요: ')
        
        emotion(str3) 
        
    elif num == 4:

        wordcloud2()
        
    elif num == 5:
        str4 = input('▶ 키워드를 입력하세요: ')

        youtube_scroll(str4)

    elif  num ==6 :

        str4 = input('▶ 키워드를 입력하세요: ')

        google_image(str4)


    elif  num ==7 :

        str5 = input('▶ 키워드를 입력하세요: ')

        bing_image(str5)

    elif  num == 8 :
        table_name = input('result 변수에 담을 테이블명을 입력하세요')
        global result
        result = mysql_connect(table_name)
        print(result)
        
    elif  num == 9 :
        table_name = input('result2 변수에 담을 테이블명을 입력하세요')
        global result2
        result2 = mysql_connect(table_name)
        print(result2)
```


```
def choongang(keyword,num):

    from selenium.webdriver.common.keys import Keys
    from  selenium  import  webdriver  #  컴퓨터가 알아서 웹페이지를 움직일 수 있도록 하는 모듈
    from  bs4  import  BeautifulSoup
    import  urllib
    import   re
    import time

    list_url ="https://www.joongang.co.kr/search/news?keyword=" + keyword

    driver = webdriver.Chrome("C:\\data\\chromedriver")
    driver.implicitly_wait(10)
    driver.get(list_url)

    #더보기 4번 실행하도록
    for i in  range(1, num+1): 

        더보기 = driver.find_element_by_css_selector('a.btn.btn_outline_gray')  # 더보기 버튼의 자바스크립트 함수명
        더보기.send_keys('\n')  # 엔터를 칩니다.
        time.sleep(3)      # 기사가 로딩되는 시간이 필요하므로 3초 기다립니다.
        driver.find_element_by_xpath("//body").send_keys(Keys.PAGE_DOWN)  # 페이지 다운 합니다.

    #스크롤을 내려 가져온 html 가져온다, 뷰티불 스프에서 읽도록 파싱한다
    html = driver.page_source
    soup = BeautifulSoup(  html, "html.parser") 

    #가져올 부분의 위치
    base=soup.select("h2.headline > a")
    #담을 params 생성
    params=[]

    #상위,하위 5개는 인공지능과 관련이 없음으로 제외,그외 href 클래스에 속한 url을 params에 append
    for i in base[5:-5]:
        params.append(i.get("href"))
    #len함수로 갯수 파악해보기
    #print(len(params))

    #저장하기
    f2 = open('C:\\data\\/choongang2.txt', "w", encoding="utf8") 
    cnt=0  

    #params의 url(k) 도 파이썬이 읽을 수 있게 전처리          
    for k in params:        
        url=urllib.request.Request(k)
        f=urllib.request.urlopen(url).read().decode("utf8")

        soup=BeautifulSoup(f,'html.parser')

        for i in soup.find_all("div",class_="article_body fs3"):
            cnt+=1      
            f2.write(str(cnt)+re.sub('[\r\t]','',i.text)+'\n')

    f2.close()
```


```
def naver_blog(keyword, num): 

    import urllib.request 
    from  bs4 import BeautifulSoup 
    from selenium import webdriver   
    from selenium.webdriver.common.keys import Keys 
    import time
    

    # 웹브라우져로 크롬을 사용할거라서 크롬 드라이버를 다운받아 위의 위치에 둔다 
    binary = "C:\\data\\chromedriver"

    # 브라우져를 인스턴스화 
    driver= webdriver.Chrome(binary) 


    params=[]           # 상세 url 담기 위한 리스트를 생성  
    for i in range(1,num+1):  # 5번만 반복하는데  

        url="https://section.blog.naver.com/Search/Post.naver?pageNo=" + str(i) + "&rangeType=ALL&orderBy=sim&keyword=" + keyword 

        # 네이버 블러그 검색 url 받아옴(검색창 비어있을때 url) 
        driver.get(url)  # 이렇게 가져와야 해당 html 페이지를 온전히 가져올 수 있습니다.  

        time.sleep(1)   

        html = driver.page_source   # 크롬브라우져에서 현재 불러온 html 소스를 가져옵니다 

        soup = BeautifulSoup(html, "html.parser") # html 코드를 검색할 수 있도록 설정, html.parser 로 해도 되면 이렇게 해도 됩니다.  

        base= soup.select('div.desc > a')  #  div 테그의 desc 클래스의 자식 테그인 a 테그에 있는 html 코드들을 가져옵니다.  


        for  i  in  base: 
            params.append(i.get('ng-href')) 

    params2 = set(params)  # params 리스트에 있는 중복된 요소를 제거합니다.  


    import  re
    params3 = list(params2) 

    f2 = open('C:\\data\\'+keyword+'.txt', "w", encoding="utf8") 
    cnt = 0 

    driver2= webdriver.Chrome(binary) 

    for  k  in  params3: 

        driver2.get(k) 

        time.sleep(5) 

        element = driver2.find_element_by_id("mainFrame") #iframe 태그 엘리먼트 찾기 

        driver2.switch_to.frame(element)  # 이렇게 스위치해줘야 확장되어서 안으로 들어갈 수 있습니다. 

        html = driver2.page_source   # 크롬브라우져에서 현재 불러온 html 소스를 가져옵니다 

        # 가져온 html 을 beautifulsoup 이 인식할 수 있도록 파싱합니다. 
        soup = BeautifulSoup(  html, "html.parser") 

        base2 = soup.select("div.se-component-content > div > div > p") 

        for  i  in   base2: 
            if len(re.sub ('[\w\r\t]', ' ', i.text)) > 1:

                cnt += 1 
                f2.write( str(cnt) + '.   ' +  re.sub('[\n\r\t]', '',i.text ) + '\n' ) 

    f2.close()
```


```
def emotion(keyword):

    textt = open("C:\\data\\" + keyword, encoding='utf-8-sig')
    positive= open("C:\\data\\positive_kor.txt", encoding="utf8")
    nagative= open("C:\\data\\nagative_kor.txt", encoding="utf8")

    n=set(nagative.read().split('\n'))    
    p=set(positive.read().split('\n'))    
    t=textt.read()

    p = list(filter(lambda x:x,p))
    p = list( filter(lambda  x : True if len(x) > 1 else False, p))

    n = list(filter(lambda x:x,n))
    n = list( filter(lambda  x : True if len(x) > 1 else False, n)) 

    f1=open('C:\\data\\'+ keyword +'pos.csv',"w", encoding="euckr")

    p.remove('ㅎㅎ') 
    p.remove('^^') 
    p.remove('이벤트') 

    f2=open('C:\\data\\'+ keyword +'nag.csv',"w", encoding="euckr")

    n.remove('ㅜㅜ ') 
    n.remove('ㅠㅠ ') 
    n.remove(':)') # 스마일 페이스 
    n.remove(':/') 
    n.remove('저는') 
    n.remove('마약') 
    n.append('느끼') 
    n.append('물리') 

    for i in p:
        if i in t:
            f1.write(i+','+str(t.count(i))+'\n')

    f1.close() 

    for i in n:
        if i in t:
            f2.write(i+','+str(t.count(i))+'\n')

    f2.close()


    import pandas as pd
    lp = pd.read_csv('C:\\data\\'+ keyword + 'pos.csv',encoding='euckr',header=None)
    lp.columns=['긍정단어','횟수']

    lp['순위']= lp['횟수'].rank(ascending=False).astype('int')
    a=lp[:][lp['순위']<=20].sort_values(by=['순위'])

    print(a)


    import pandas as pd
    lp = pd.read_csv('C:\\data\\'+ keyword +'nag.csv',encoding='euckr',header=None)
    lp.columns=['부정단어','횟수']

    lp['순위']= lp['횟수'].rank(ascending=False).astype('int')
    b=lp[:][lp['순위']<=20].sort_values(by=['순위'])


    print(b)
```


```
def  wordcloud2():
    
    ss = 'c:\\data\\'    
    name = input('파일명을 입력하세요 ~ ')

    # 텍스트마이닝 데이터 정제
    from wordcloud import WordCloud, STOPWORDS  # 시각화, 데이터 정제를 위해 임폴트
    import matplotlib.pyplot as plt  # 그래프 그리는 모듈
    from os import path     #  os 에 있는 파일을 파이썬에서 인식하기 위해서
    import re   #  데이터 정제를 위해서 필요한 모듈 
    import numpy as np  
    from PIL import Image  # 이미지 시각화를 위한 모듈 

    # 워드 클라우드의 배경이 되는 이미지 모양을 결정
    usa_mask = np.array(Image.open("C:\\data\\usa_im.png"))

    # 워드 클라우드를 그릴 스크립트 이름을 물어봅니다. 
    script = ss + name

    # 떡군이네 데이터 스크립트와 os 의 위치를 연결하여 utf8로 인코딩해서 한글 텍스트를
    # text 변수로 리턴한다.
    text = open( script , mode="r", encoding="utf-8").read()

    # 너무 공통적으로 자주 나오는 단어들을 제거하기 위한 작업  
    file = open('C:\\data\\word.txt', 'r', encoding = 'utf-8')
    
    word = file.read().split(' ')   # 어절별로 분리함
    for i in word:
        text = re.sub(i,'',text)      # 떡군이네 text 에서 word 에 있는 단어들을 전부 null 로 변경한다. 

    # 워드 클라우드를 그린다. 
    wordcloud = WordCloud(font_path='C://Windows//Fonts//gulim', # 글씨체
                          stopwords=STOPWORDS,   # 마침표, 느낌표,싱글 쿼테이션 등을 정제
                          max_words=1000, # 워드 클라우드에 그릴 최대 단어갯수
                          background_color='white', # 배경색깔
                          max_font_size = 100, # 최대 글씨 크기 
                          min_font_size = 1, # 최소 글씨 
                          mask = usa_mask, # 배경 모양 
                          colormap='jet').generate(text).to_file('C:\\data\\cloud.png')
                      # c 드라이브 밑에 project 폴더 밑에 생성되는 워드 클라우드 이미지 이름

    plt.figure(figsize=(15,15))            # 워드 클라우드의 가로 세로 사이즈
    plt.imshow(wordcloud, interpolation='bilinear')  # 글씨가 퍼지는 스타일 
    plt.axis("off")
```


```
#첫번째 함수 
from selenium import webdriver as wd  # 웹스크롤링을 자동화하기위한 모듈을 임폴트 합니다. 
from bs4 import BeautifulSoup   # html 문서에 원하는 부분만 가져오기 위한 모듈 
import time      #  웹 스크롤링을 할 때 중간중간 잠시 쉬어서 서버에 부담을 주지 않게 하기 위해                       
import pandas as pd  # 웹스크롤링한 결과를 예쁘게 표형태로 만들기 위한 모듈입니다. 
import requests  # 웹 url 을 파이썬이 인식할 수 있게 해주는 모듈 
import re   #  데이터 정제 전문 모듈입니다. 
```


```
# 유투브에 키워드를 넣으면 영상들의 url 가져옴
def get_urls_from_youtube_with_keyword(keyword): 
 
    titles = []       # 유튜브 영상 제목을 담기위한 리스트 변수를 만듭니다                                  
    urls = []         # 키워드를 넣었을때 나오는 모든 영상들의 url 주소를 담기 위한 리스트 만듭니다. 
     
    # 입력한 키워드를 컴퓨터가 알아들을 수 있는 언어로 인코딩합니다  
    search_keyword_encode = requests.utils.quote(keyword) 
     
    # 유튜브에서 해당 키워드의 영상 리스트를 찾을 수 있는 url 을 만들어서 url 변수에 입력합니다 
    url = "https://www.youtube.com/results?search_query=" + search_keyword_encode 
     
    # 크롬 드라이버의 위치를 지정한 후에 driver 라는 객체를 생성합니다 
    driver = wd.Chrome(executable_path="C:\\data\\chromedriver_win32\\chromedriver.exe") 
     
    # 크롬 드라이버 객체가 url 을 읽어들입니다 
    driver.get(url) 
     
    # 키워드로 받아온 영상 리스트 웹페이지의 처음부터 맨 마지막까지의 높이를 추출합니다 
    last_page_height = driver.execute_script("return document.documentElement.scrollHeight") 

    while True:  # 아래의 실행문이 무한히 반복될 수 있게 합니다 

        # 마우스 스크롤을 끝까지 내리겠금 합니다 
        driver.execute_script("window.scrollTo(0, document.documentElement.scrollHeight);") 
         
        time.sleep(3.0) # 잠깐 3초 쉽니다 
         
        # 내린 스크롤까지의 높이를 구해서  
        new_page_height = driver.execute_script("return document.documentElement.scrollHeight") 
         
        if new_page_height == last_page_height:  # 지금의 높이가 웹페이지의 끝이라면 
            break          # 종료해라 ~ 
             
        last_page_height = new_page_height  # 그렇지 않으면 계속 스크롤을 진행하게 합니다 
         
    html_source = driver.page_source  # 페이지의 html 소스를 html_source 에 담습니다 
     
    driver.quit()   # 브라우져 창을 닫습니다 
     
    soup = BeautifulSoup(html_source, 'lxml')   #  받아온 html 코드를 beautifulsoup 으로 파싱합니다 
     
    datas = soup.select("a#video-title")  # a 테그의 id 가 video-title 인곳의 html 문서를 가져옵니다 

    for data in datas: 
        title = data.text.replace('\n', '')   # 불러온 html 코드에서 엔터를 없엡니다 
        url = "https://www.youtube.com/" + data.get('href')  # 상세 url 을 가져옵니다 
         
        titles.append(title)     #  영상제목을 titles 에 append 시킵니다 
        urls.append(url)        #  상세 url 을 append 시킵니다 
         
    return titles, urls 
```


```
# 두번째 함수: 끝까지 스크롤링하고, n 개 영상 댓글에 대한 상세 주소
def crawl_youtube_page_html_sources(urls): 
    html_sources = [] 

    for i in range(0, 2): 
        driver = wd.Chrome(executable_path="C:\\data\\chromedriver_win32\\chromedriver.exe") 
        driver.get(urls[i]) 

        last_page_height = driver.execute_script("return document.documentElement.scrollHeight") 

        while True: 
            driver.execute_script("window.scrollTo(0, document.documentElement.scrollHeight);") 
            time.sleep(3.0) 
            new_page_height = driver.execute_script("return document.documentElement.scrollHeight") 

            if new_page_height == last_page_height: 
                break 
            last_page_height = new_page_height
            
            driver.execute_script('window.scrollTo(0,500)')
            time.sleep(2)

        #페이지의 html 소스를 html_source 에 담는다    
        html_source = driver.page_source 
        html_sources.append(html_source) 
        print("OK💘") 

        driver.quit() 
    return html_sources 
```


```
# 세번째 함수:  글쓴이, 댓글 불러오기
def get_user_IDs_and_comments(html_sources): 

    import re 
    my_dataframes = []  # 뎃글 데이터를 저장할 판다스 데이터 프레임명을 저장할 리스트를 생성 

    #for html in html_sources: # html_sources 리스트 안에 있는 html 코드들을 차례데로 가져옵니다 
    cnt = 0 
    
    while True:      
        html = html_sources[cnt] 
        cnt += 1 
         
        soup = BeautifulSoup(html, 'lxml')  #  lxml 로 안되면 html.parser 라고 하면 됩니다 
         
        youtube_user_IDs = soup.select('div#header-author > h3 > a > span')  # 글쓴이 
        #youtube_user_IDs = soup.select('span.style-scope.ytd-comment-renderer')  
        youtube_comments = soup.select('yt-formatted-string#content-text') # 뎃글 
        
        #글쓴이 리스트 저장을 위한 리스트
        str_youtube_userIDs = [] 
        for i in range(len(youtube_user_IDs)): 
            str_tmp = str(youtube_user_IDs[i].text) 
            str_youtube_userIDs.append(re.sub('[\n\r\t]','',str_tmp)) 
        
        #댓글 저장 리스트    
        str_youtube_comments = []     
        for i in range(len(youtube_comments)): 
            str_tmp = str(youtube_comments[i].text) 
            str_youtube_comments.append(re.sub('[\n\r\t]','',str_tmp))             

            
        #글쓴이 리스트와 댓글 리스트로 딕션어리를 만들어준다  
        # id,comment--->컬럼 /  안에 들어있는 내용들--->데이터
        pd_data = {"ID":str_youtube_userIDs, "Comment":str_youtube_comments} 

        youtube_pd = pd.DataFrame(pd_data) 

        my_dataframes.append(youtube_pd) 
         
        print('ok😈') 
        if  cnt ==2: 
            break 
         
    return my_dataframes 
```

네번째 함수 : 저장 


```
def convert_csv_from_dataframe(titles, my_dataframes): 
    for i in range(len(my_dataframes)): 
        title = re.sub('[-=+,#/\?:^$.@*\"※~&%ㆍ!』\\‘|\(\)\[\]\<\>`\'…《\》]', '', titles[i]) 
        my_dataframes[i].to_csv("C:\\data\\{}.csv".format(title)) 
```

2. 4개의 함수를 실행하는 코드를 가지고 함수를 생성합니다.  


```
def  youtube_scroll(keyword): 
    title, urls = get_urls_from_youtube_with_keyword(keyword) 
    html_sources =crawl_youtube_page_html_sources(urls) 
    my_dataframes = get_user_IDs_and_comments(html_sources) 
    convert_csv_from_dataframe(title, my_dataframes)   
# 구글 이미지 스크롤
def  google_image(keyword):
    ######################################################

    import urllib.request            # 파이썬에서 웹의 url 을 인식할 수 있게하는 모듈
    from  bs4 import BeautifulSoup
    from selenium import webdriver  # 크롬 웹브라우져를 자동으로 제어하기 위해 
    from selenium.webdriver.common.keys import Keys  # 키보드를 컴퓨터가 알아서 누르기 위해서
    import time                     # 중간중간 sleep 을 걸어야 해서 time 모듈 import
    
    
    binary = "C:\\data\\chromedriver_win32\\chromedriver.exe"

    # 브라우져를 인스턴스화
    browser = webdriver.Chrome(binary)

    # 구글의 이미지 검색 url 받아옴(키워드를 아무것도 안 쳤을때의 url)
    browser.get("https://www.google.co.kr/imghp?hl=ko&tab=wi&ei=l1AdWbegOcra8QXvtr-4Cw&ved=0EKouCBUoAQ")

    # 검색창에 검색 키워드를 넣기 위해서 웹페이지의 검색창의 클래스 이름을 찾아서 검색창에 해당하는 부분이 
    # 어디다라고 알려주는 elem 객체를 만듭니다.

    elem = browser.find_element_by_xpath("//*[@class='gLFyf gsfi']") 

    ########################### 검색어 입력 ###########################

    # elem 이 input 창과 연결되어 스스로 햄버거를 검색
    elem.send_keys(keyword)

    # 웹에서의 submit 은 엔터의 역할을 함
    elem.submit()

    ########################### 반복할 횟수 ###########################

    # 스크롤을 내리려면 브라우져 이미지 검색결과 부분(바디부분)에 마우스 클릭 한번 하고 End키를 눌러야함
    for i in range(1, 10):
        browser.find_element_by_xpath("//body").send_keys(Keys.END)
        time.sleep(10)                  # END 키 누르고 내려가는데 시간이 걸려서 sleep 해줌

    browser.find_element_by_xpath("//*[@class='mye4qd']").send_keys(Keys.ENTER)  # 더보기 누름

    for i in range(1, 5):
        browser.find_element_by_xpath("//body").send_keys(Keys.END)
        time.sleep(10)

    time.sleep(10)                      # 네트워크 느릴까봐 안정성 위해 sleep 해줌
    html = browser.page_source         # 크롬브라우져에서 현재 불러온 소스 가져옴
    soup = BeautifulSoup(html, "lxml") # html 코드를 검색할 수 있도록 설정


    ########################### 그림파일 저장 ###########################

    def fetch_list_url():  # 이미지에 대한 상세 url 을 params 리스트에 담는 함수
        params = []      
        imgList = soup.find_all("img", class_="rg_i Q4LuWd")  # 구글 이미지 url 이 있는 img 태그의 _img 클래스에 가서
        for im in imgList:
            try :
                params.append(im["src"])                   # params 리스트에 image url 을 담음
            except KeyError:
                params.append(im["data-src"])  # src 에서 못가져오면 data-src 에서 가져와라
        return params

    def   fetch_detail_url():     # 상세 url 의 이미지들을 우리 컴퓨터에 다운로드 받는 함수
        params = fetch_list_url()  # fetch_list_url() 함수를 실행하고 params 리스트에  상세 url 을 넣습니다.

        for idx, p in enumerate(params,1):                      
            urllib.request.urlretrieve( p , "/Users/munhuijung/googleimage/" + str(idx) + ".jpg")

    # enumerate 는 리스트의 모든 요소를 인덱스와 쌍으로 추출
    # 하는 함수 . 숫자 1은 인덱스를 1부터 시작해라 ~

    fetch_detail_url()

    # 끝나면 브라우져 닫기
    browser.quit()
```


```
# bing 이미지 검색
def bing_image(keyword):

    ######################################################

    import urllib.request            # 파이썬에서 웹의 url 을 인식할 수 있게하는 모듈
    from  bs4 import BeautifulSoup
    from selenium import webdriver  # 크롬 웹브라우져를 자동으로 제어하기 위해 
    from selenium.webdriver.common.keys import Keys  # 키보드를 컴퓨터가 알아서 누르기 위해서
    import time                     # 중간중간 sleep 을 걸어야 해서 time 모듈 import

    ########################### url 받아오기 ###########################

    # 웹브라우져로 크롬을 사용할거라서 크롬 드라이버를 다운받아 위의 위치에 둔다
    # 팬텀 js로 하면 백그라운드로 실행할 수 있음

    binary = "C:\\data\\chromedriver_win32\\chromedriver.exe"

    # 브라우져를 인스턴스화
    browser = webdriver.Chrome(binary)

    # 빙 이미지 검색 url 받아옴(키워드를 아무것도 안 쳤을때의 url)
    browser.get("https://www.bing.com/?scope=images&nr=1&FORM=NOFORM")
    time.sleep(5)

    # 검색창에 검색 키워드를 넣기 위해서 웹페이지의 검색창의 클래스 이름을 찾아서 검색창에 해당하는 부분이 
    # 어디다라고 알려주는 elem 객체를 만듭니다.

    elem = browser.find_element_by_id("sb_form_q") 

    ########################### 검색어 입력 ###########################

    # elem 이 input 창과 연결되어 스스로 햄버거를 검색
    elem.send_keys(keyword)

    # 웹에서의 submit 은 엔터의 역할을 함
    elem.submit()

    ########################### 반복할 횟수 ###########################

    # 스크롤을 내리려면 브라우져 이미지 검색결과 부분(바디부분)에 마우스 클릭 한번 하고 End키를 눌러야함
    for i in range(1, 10):
        browser.find_element_by_xpath("//body").send_keys(Keys.END)
        time.sleep(10)                  # END 키 누르고 내려가는데 시간이 걸려서 sleep 해줌


    time.sleep(10)                      # 네트워크 느릴까봐 안정성 위해 sleep 해줌
    html = browser.page_source         # 크롬브라우져에서 현재 불러온 소스 가져옴
    soup = BeautifulSoup(html, "lxml") # html 코드를 검색할 수 있도록 설정


    ########################### 그림파일 저장 ###########################

    def fetch_list_url():  # 이미지에 대한 상세 url 을 params 리스트에 담는 함수
        params = []      
        imgList = soup.find_all("img", class_="mimg")  # 빙 이미지 url 이 있는 img 태그의 mimg 클래스에 가서
        for im in imgList:
            try :
                params.append(im["src"])                   # params 리스트에 image url 을 담음
            except KeyError:
                params.append(im["data-src"])  # src 에서 못가져오면 data-src 에서 가져와라
        return params

    def   fetch_detail_url():     # 상세 url 의 이미지들을 우리 컴퓨터에 다운로드 받는 함수
        params = fetch_list_url()  # fetch_list_url() 함수를 실행하고 params 리스트에  상세 url 을 넣습니다.

        for   idx ,  p  in enumerate(params,1):  #  params 리스트의 요소를 하나씩 가져오는데 번호를 부여해서 가져
            # 다운받을 폴더경로 입력              # 옵니다. 그런데 1번부터 부여합니다.
            urllib.request.urlretrieve( p ,"C:\data\bing" + str(idx) + ".jpg")

    # enumerate 는 리스트의 모든 요소를 인덱스와 쌍으로 추출
    # 하는 함수 . 숫자 1은 인덱스를 1부터 시작해라 ~

    fetch_detail_url()

    # 끝나면 브라우져 닫기
    browser.quit()
def mysql_connect(table_name):
    import  pymysql
    import  pandas  as  pd 

    conn = pymysql.connect( host = "localhost", user="root", password="ansgml1201.",
                                      db ="orcl", charset ="utf8")
    cursors = conn.cursor()  # mySQL 에 접속한수 생성된 메모리 이름을 cursors 로 하겠다
    sql = "select * from " + table_name
    cursors.execute(sql)    #  sql 문장을 실행해서 결과를 cursors 라는 메모리에 담습니다
    rows = cursors.fetchall()   # 메모리에 있는 결과를 rows 에 넣습니다

    colname = cursors.description     #emp 테이블의 컬럼이름이 포함된 정보를 colname 에 담음
    col = []
    for  i  in  colname:
        col.append( i[0].lower() )  # emp 테이블의 컬럼이름을 소문자로 변경해서 col 리스트에 담음

    dataframe = pd.DataFrame( rows, columns = col )
    return  dataframe
```


```
result = None   # 전역변수를 선언합니다.
```


```
def Oracle_connect(table_name):
    table_name = input('result에 담고 싶은 테이블명')
    import cx_Oracle
    import pandas as pd

    dsn = cx_Oracle.makedsn("localhost",1521,"xe")
    db = cx_Oracle.connect("c##scott","tiger","dsn")
    coursor = db.cursor()
    cursor.excute('select*from'+table_name)
    rows = cursor2.fetchall()
    emp = pd.DataFrame(rows)
    colname = cursor.description
    col = []
    for i in colname:
        col.append(i[0].lower())
        
    result2 = pd.DataFrame(rows, columns = col)
    return result2 
```


```
result2=None
```
