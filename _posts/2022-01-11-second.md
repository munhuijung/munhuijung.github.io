```
#ë©”ì¸í•¨ìˆ˜     
def my_data():
    print('='*15,'ë°ì´í„° ë¶„ì„ì„ ì •ë§ í¸í•˜ê²Œ','='*15)
    print('1. ì¤‘ì•™ì¼ë³´ ê¸°ì‚¬ ë°ì´í„° ìŠ¤í¬ë¡¤ë§ \n2. ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë°ì´í„° ìŠ¤í¬ë¡¤ë§ \n3. ê¸ì •, ë¶€ì • ê°ì„±ë¶„ì„ \n4. ì›Œë“œí´ë¼ìš°ë“œ ê·¸ë¦¬ê¸° \n5. ìœ íŠœë¸Œ ìŠ¤í¬ë¡¤ë§ \n6. êµ¬ê¸€ ì´ë¯¸ì§€ ìŠ¤í¬ë¡¤ \n7. ë¹™ ì´ë¯¸ì§€ ìŠ¤í¬ë¡¤ \n8. mySQL ê³¼ ì—°ë™í•˜ê¸° \n9. Oracle ê³¼ ì—°ë™í•˜ê¸°')
    print('='*53)

    num = int(input('â–¶ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš”: '))
    
    if num == 1:
        str1 = input('â–¶ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ')
        num1 = int(input('â–¶ ë”ë³´ê¸°ë¥¼ ëª‡ ë²ˆ ëˆ„ë¥¼ê¹Œìš”?'))
        
        choongang(str1, num1)

    elif num == 2:
        str2 = input('â–¶ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ')
        num2 = int(input('â–¶ ë”ë³´ê¸°ë¥¼ ëª‡ ë²ˆ ëˆ„ë¥¼ê¹Œìš”?', 'í•œ í˜ì´ì§€ì— 7ê°œì˜ ë¸”ë¡œê·¸ê°€ ìˆìŠµë‹ˆë‹¤.'))
        
        naver_blog(str2, num2)

    elif num == 3:    
        str3 = input('â–¶ íŒŒì¼ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: ')
        
        emotion(str3) 
        
    elif num == 4:

        wordcloud2()
        
    elif num == 5:
        str4 = input('â–¶ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ')

        youtube_scroll(str4)

    elif  num ==6 :

        str4 = input('â–¶ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ')

        google_image(str4)


    elif  num ==7 :

        str5 = input('â–¶ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ')

        bing_image(str5)

    elif  num == 8 :
        table_name = input('result ë³€ìˆ˜ì— ë‹´ì„ í…Œì´ë¸”ëª…ì„ ì…ë ¥í•˜ì„¸ìš”')
        global result
        result = mysql_connect(table_name)
        print(result)
        
    elif  num == 9 :
        table_name = input('result2 ë³€ìˆ˜ì— ë‹´ì„ í…Œì´ë¸”ëª…ì„ ì…ë ¥í•˜ì„¸ìš”')
        global result2
        result2 = mysql_connect(table_name)
        print(result2)
```


```
def choongang(keyword,num):

    from selenium.webdriver.common.keys import Keys
    from  selenium  import  webdriver  #  ì»´í“¨í„°ê°€ ì•Œì•„ì„œ ì›¹í˜ì´ì§€ë¥¼ ì›€ì§ì¼ ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ëª¨ë“ˆ
    from  bs4  import  BeautifulSoup
    import  urllib
    import   re
    import time

    list_url ="https://www.joongang.co.kr/search/news?keyword=" + keyword

    driver = webdriver.Chrome("C:\\data\\chromedriver")
    driver.implicitly_wait(10)
    driver.get(list_url)

    #ë”ë³´ê¸° 4ë²ˆ ì‹¤í–‰í•˜ë„ë¡
    for i in  range(1, num+1): 

        ë”ë³´ê¸° = driver.find_element_by_css_selector('a.btn.btn_outline_gray')  # ë”ë³´ê¸° ë²„íŠ¼ì˜ ìë°”ìŠ¤í¬ë¦½íŠ¸ í•¨ìˆ˜ëª…
        ë”ë³´ê¸°.send_keys('\n')  # ì—”í„°ë¥¼ ì¹©ë‹ˆë‹¤.
        time.sleep(3)      # ê¸°ì‚¬ê°€ ë¡œë”©ë˜ëŠ” ì‹œê°„ì´ í•„ìš”í•˜ë¯€ë¡œ 3ì´ˆ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
        driver.find_element_by_xpath("//body").send_keys(Keys.PAGE_DOWN)  # í˜ì´ì§€ ë‹¤ìš´ í•©ë‹ˆë‹¤.

    #ìŠ¤í¬ë¡¤ì„ ë‚´ë ¤ ê°€ì ¸ì˜¨ html ê°€ì ¸ì˜¨ë‹¤, ë·°í‹°ë¶ˆ ìŠ¤í”„ì—ì„œ ì½ë„ë¡ íŒŒì‹±í•œë‹¤
    html = driver.page_source
    soup = BeautifulSoup(  html, "html.parser") 

    #ê°€ì ¸ì˜¬ ë¶€ë¶„ì˜ ìœ„ì¹˜
    base=soup.select("h2.headline > a")
    #ë‹´ì„ params ìƒì„±
    params=[]

    #ìƒìœ„,í•˜ìœ„ 5ê°œëŠ” ì¸ê³µì§€ëŠ¥ê³¼ ê´€ë ¨ì´ ì—†ìŒìœ¼ë¡œ ì œì™¸,ê·¸ì™¸ href í´ë˜ìŠ¤ì— ì†í•œ urlì„ paramsì— append
    for i in base[5:-5]:
        params.append(i.get("href"))
    #lení•¨ìˆ˜ë¡œ ê°¯ìˆ˜ íŒŒì•…í•´ë³´ê¸°
    #print(len(params))

    #ì €ì¥í•˜ê¸°
    f2 = open('C:\\data\\/choongang2.txt', "w", encoding="utf8") 
    cnt=0  

    #paramsì˜ url(k) ë„ íŒŒì´ì¬ì´ ì½ì„ ìˆ˜ ìˆê²Œ ì „ì²˜ë¦¬          
    for k in params:        
        url=urllib.request.Request(k)
        f=urllib.request.urlopen(url).read().decode("utf8")

        soup=BeautifulSoup(f,'html.parser')

        for i in soup.find_all("div",class_="article_body fs3"):
            cnt+=1      
            f2.write(str(cnt)+re.sub('[\r\t]','',i.text)+'\n')

    f2.close()
```


```
def naver_blog(keyword, num): 

    import urllib.request 
    from  bs4 import BeautifulSoup 
    from selenium import webdriver   
    from selenium.webdriver.common.keys import Keys 
    import time
    

    # ì›¹ë¸Œë¼ìš°ì ¸ë¡œ í¬ë¡¬ì„ ì‚¬ìš©í• ê±°ë¼ì„œ í¬ë¡¬ ë“œë¼ì´ë²„ë¥¼ ë‹¤ìš´ë°›ì•„ ìœ„ì˜ ìœ„ì¹˜ì— ë‘”ë‹¤ 
    binary = "C:\\data\\chromedriver"

    # ë¸Œë¼ìš°ì ¸ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™” 
    driver= webdriver.Chrome(binary) 


    params=[]           # ìƒì„¸ url ë‹´ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±  
    for i in range(1,num+1):  # 5ë²ˆë§Œ ë°˜ë³µí•˜ëŠ”ë°  

        url="https://section.blog.naver.com/Search/Post.naver?pageNo=" + str(i) + "&rangeType=ALL&orderBy=sim&keyword=" + keyword 

        # ë„¤ì´ë²„ ë¸”ëŸ¬ê·¸ ê²€ìƒ‰ url ë°›ì•„ì˜´(ê²€ìƒ‰ì°½ ë¹„ì–´ìˆì„ë•Œ url) 
        driver.get(url)  # ì´ë ‡ê²Œ ê°€ì ¸ì™€ì•¼ í•´ë‹¹ html í˜ì´ì§€ë¥¼ ì˜¨ì „íˆ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

        time.sleep(1)   

        html = driver.page_source   # í¬ë¡¬ë¸Œë¼ìš°ì ¸ì—ì„œ í˜„ì¬ ë¶ˆëŸ¬ì˜¨ html ì†ŒìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ 

        soup = BeautifulSoup(html, "html.parser") # html ì½”ë“œë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •, html.parser ë¡œ í•´ë„ ë˜ë©´ ì´ë ‡ê²Œ í•´ë„ ë©ë‹ˆë‹¤.  

        base= soup.select('div.desc > a')  #  div í…Œê·¸ì˜ desc í´ë˜ìŠ¤ì˜ ìì‹ í…Œê·¸ì¸ a í…Œê·¸ì— ìˆëŠ” html ì½”ë“œë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.  


        for  i  in  base: 
            params.append(i.get('ng-href')) 

    params2 = set(params)  # params ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ì¤‘ë³µëœ ìš”ì†Œë¥¼ ì œê±°í•©ë‹ˆë‹¤.  


    import  re
    params3 = list(params2) 

    f2 = open('C:\\data\\'+keyword+'.txt', "w", encoding="utf8") 
    cnt = 0 

    driver2= webdriver.Chrome(binary) 

    for  k  in  params3: 

        driver2.get(k) 

        time.sleep(5) 

        element = driver2.find_element_by_id("mainFrame") #iframe íƒœê·¸ ì—˜ë¦¬ë¨¼íŠ¸ ì°¾ê¸° 

        driver2.switch_to.frame(element)  # ì´ë ‡ê²Œ ìŠ¤ìœ„ì¹˜í•´ì¤˜ì•¼ í™•ì¥ë˜ì–´ì„œ ì•ˆìœ¼ë¡œ ë“¤ì–´ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

        html = driver2.page_source   # í¬ë¡¬ë¸Œë¼ìš°ì ¸ì—ì„œ í˜„ì¬ ë¶ˆëŸ¬ì˜¨ html ì†ŒìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ 

        # ê°€ì ¸ì˜¨ html ì„ beautifulsoup ì´ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ íŒŒì‹±í•©ë‹ˆë‹¤. 
        soup = BeautifulSoup(  html, "html.parser") 

        base2 = soup.select("div.se-component-content > div > div > p") 

        for  i  in   base2: 
            if len(re.sub ('[\w\r\t]', ' ', i.text)) > 1:

                cnt += 1 
                f2.write( str(cnt) + '.   ' +  re.sub('[\n\r\t]', '',i.text ) + '\n' ) 

    f2.close()
```


```
def emotion(keyword):

    textt = open("C:\\data\\" + keyword, encoding='utf-8-sig')
    positive= open("C:\\data\\positive_kor.txt", encoding="utf8")
    nagative= open("C:\\data\\nagative_kor.txt", encoding="utf8")

    n=set(nagative.read().split('\n'))    
    p=set(positive.read().split('\n'))    
    t=textt.read()

    p = list(filter(lambda x:x,p))
    p = list( filter(lambda  x : True if len(x) > 1 else False, p))

    n = list(filter(lambda x:x,n))
    n = list( filter(lambda  x : True if len(x) > 1 else False, n)) 

    f1=open('C:\\data\\'+ keyword +'pos.csv',"w", encoding="euckr")

    p.remove('ã…ã…') 
    p.remove('^^') 
    p.remove('ì´ë²¤íŠ¸') 

    f2=open('C:\\data\\'+ keyword +'nag.csv',"w", encoding="euckr")

    n.remove('ã…œã…œ ') 
    n.remove('ã… ã…  ') 
    n.remove(':)') # ìŠ¤ë§ˆì¼ í˜ì´ìŠ¤ 
    n.remove(':/') 
    n.remove('ì €ëŠ”') 
    n.remove('ë§ˆì•½') 
    n.append('ëŠë¼') 
    n.append('ë¬¼ë¦¬') 

    for i in p:
        if i in t:
            f1.write(i+','+str(t.count(i))+'\n')

    f1.close() 

    for i in n:
        if i in t:
            f2.write(i+','+str(t.count(i))+'\n')

    f2.close()


    import pandas as pd
    lp = pd.read_csv('C:\\data\\'+ keyword + 'pos.csv',encoding='euckr',header=None)
    lp.columns=['ê¸ì •ë‹¨ì–´','íšŸìˆ˜']

    lp['ìˆœìœ„']= lp['íšŸìˆ˜'].rank(ascending=False).astype('int')
    a=lp[:][lp['ìˆœìœ„']<=20].sort_values(by=['ìˆœìœ„'])

    print(a)


    import pandas as pd
    lp = pd.read_csv('C:\\data\\'+ keyword +'nag.csv',encoding='euckr',header=None)
    lp.columns=['ë¶€ì •ë‹¨ì–´','íšŸìˆ˜']

    lp['ìˆœìœ„']= lp['íšŸìˆ˜'].rank(ascending=False).astype('int')
    b=lp[:][lp['ìˆœìœ„']<=20].sort_values(by=['ìˆœìœ„'])


    print(b)
```


```
def  wordcloud2():
    
    ss = 'c:\\data\\'    
    name = input('íŒŒì¼ëª…ì„ ì…ë ¥í•˜ì„¸ìš” ~ ')

    # í…ìŠ¤íŠ¸ë§ˆì´ë‹ ë°ì´í„° ì •ì œ
    from wordcloud import WordCloud, STOPWORDS  # ì‹œê°í™”, ë°ì´í„° ì •ì œë¥¼ ìœ„í•´ ì„í´íŠ¸
    import matplotlib.pyplot as plt  # ê·¸ë˜í”„ ê·¸ë¦¬ëŠ” ëª¨ë“ˆ
    from os import path     #  os ì— ìˆëŠ” íŒŒì¼ì„ íŒŒì´ì¬ì—ì„œ ì¸ì‹í•˜ê¸° ìœ„í•´ì„œ
    import re   #  ë°ì´í„° ì •ì œë¥¼ ìœ„í•´ì„œ í•„ìš”í•œ ëª¨ë“ˆ 
    import numpy as np  
    from PIL import Image  # ì´ë¯¸ì§€ ì‹œê°í™”ë¥¼ ìœ„í•œ ëª¨ë“ˆ 

    # ì›Œë“œ í´ë¼ìš°ë“œì˜ ë°°ê²½ì´ ë˜ëŠ” ì´ë¯¸ì§€ ëª¨ì–‘ì„ ê²°ì •
    usa_mask = np.array(Image.open("C:\\data\\usa_im.png"))

    # ì›Œë“œ í´ë¼ìš°ë“œë¥¼ ê·¸ë¦´ ìŠ¤í¬ë¦½íŠ¸ ì´ë¦„ì„ ë¬¼ì–´ë´…ë‹ˆë‹¤. 
    script = ss + name

    # ë–¡êµ°ì´ë„¤ ë°ì´í„° ìŠ¤í¬ë¦½íŠ¸ì™€ os ì˜ ìœ„ì¹˜ë¥¼ ì—°ê²°í•˜ì—¬ utf8ë¡œ ì¸ì½”ë”©í•´ì„œ í•œê¸€ í…ìŠ¤íŠ¸ë¥¼
    # text ë³€ìˆ˜ë¡œ ë¦¬í„´í•œë‹¤.
    text = open( script , mode="r", encoding="utf-8").read()

    # ë„ˆë¬´ ê³µí†µì ìœ¼ë¡œ ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ì„ ì œê±°í•˜ê¸° ìœ„í•œ ì‘ì—…  
    file = open('C:\\data\\word.txt', 'r', encoding = 'utf-8')
    
    word = file.read().split(' ')   # ì–´ì ˆë³„ë¡œ ë¶„ë¦¬í•¨
    for i in word:
        text = re.sub(i,'',text)      # ë–¡êµ°ì´ë„¤ text ì—ì„œ word ì— ìˆëŠ” ë‹¨ì–´ë“¤ì„ ì „ë¶€ null ë¡œ ë³€ê²½í•œë‹¤. 

    # ì›Œë“œ í´ë¼ìš°ë“œë¥¼ ê·¸ë¦°ë‹¤. 
    wordcloud = WordCloud(font_path='C://Windows//Fonts//gulim', # ê¸€ì”¨ì²´
                          stopwords=STOPWORDS,   # ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ,ì‹±ê¸€ ì¿¼í…Œì´ì…˜ ë“±ì„ ì •ì œ
                          max_words=1000, # ì›Œë“œ í´ë¼ìš°ë“œì— ê·¸ë¦´ ìµœëŒ€ ë‹¨ì–´ê°¯ìˆ˜
                          background_color='white', # ë°°ê²½ìƒ‰ê¹”
                          max_font_size = 100, # ìµœëŒ€ ê¸€ì”¨ í¬ê¸° 
                          min_font_size = 1, # ìµœì†Œ ê¸€ì”¨ 
                          mask = usa_mask, # ë°°ê²½ ëª¨ì–‘ 
                          colormap='jet').generate(text).to_file('C:\\data\\cloud.png')
                      # c ë“œë¼ì´ë¸Œ ë°‘ì— project í´ë” ë°‘ì— ìƒì„±ë˜ëŠ” ì›Œë“œ í´ë¼ìš°ë“œ ì´ë¯¸ì§€ ì´ë¦„

    plt.figure(figsize=(15,15))            # ì›Œë“œ í´ë¼ìš°ë“œì˜ ê°€ë¡œ ì„¸ë¡œ ì‚¬ì´ì¦ˆ
    plt.imshow(wordcloud, interpolation='bilinear')  # ê¸€ì”¨ê°€ í¼ì§€ëŠ” ìŠ¤íƒ€ì¼ 
    plt.axis("off")
```


```
#ì²«ë²ˆì§¸ í•¨ìˆ˜ 
from selenium import webdriver as wd  # ì›¹ìŠ¤í¬ë¡¤ë§ì„ ìë™í™”í•˜ê¸°ìœ„í•œ ëª¨ë“ˆì„ ì„í´íŠ¸ í•©ë‹ˆë‹¤. 
from bs4 import BeautifulSoup   # html ë¬¸ì„œì— ì›í•˜ëŠ” ë¶€ë¶„ë§Œ ê°€ì ¸ì˜¤ê¸° ìœ„í•œ ëª¨ë“ˆ 
import time      #  ì›¹ ìŠ¤í¬ë¡¤ë§ì„ í•  ë•Œ ì¤‘ê°„ì¤‘ê°„ ì ì‹œ ì‰¬ì–´ì„œ ì„œë²„ì— ë¶€ë‹´ì„ ì£¼ì§€ ì•Šê²Œ í•˜ê¸° ìœ„í•´                       
import pandas as pd  # ì›¹ìŠ¤í¬ë¡¤ë§í•œ ê²°ê³¼ë¥¼ ì˜ˆì˜ê²Œ í‘œí˜•íƒœë¡œ ë§Œë“¤ê¸° ìœ„í•œ ëª¨ë“ˆì…ë‹ˆë‹¤. 
import requests  # ì›¹ url ì„ íŒŒì´ì¬ì´ ì¸ì‹í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ëª¨ë“ˆ 
import re   #  ë°ì´í„° ì •ì œ ì „ë¬¸ ëª¨ë“ˆì…ë‹ˆë‹¤. 
```


```
# ìœ íˆ¬ë¸Œì— í‚¤ì›Œë“œë¥¼ ë„£ìœ¼ë©´ ì˜ìƒë“¤ì˜ url ê°€ì ¸ì˜´
def get_urls_from_youtube_with_keyword(keyword): 
 
    titles = []       # ìœ íŠœë¸Œ ì˜ìƒ ì œëª©ì„ ë‹´ê¸°ìœ„í•œ ë¦¬ìŠ¤íŠ¸ ë³€ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤                                  
    urls = []         # í‚¤ì›Œë“œë¥¼ ë„£ì—ˆì„ë•Œ ë‚˜ì˜¤ëŠ” ëª¨ë“  ì˜ìƒë“¤ì˜ url ì£¼ì†Œë¥¼ ë‹´ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸ ë§Œë“­ë‹ˆë‹¤. 
     
    # ì…ë ¥í•œ í‚¤ì›Œë“œë¥¼ ì»´í“¨í„°ê°€ ì•Œì•„ë“¤ì„ ìˆ˜ ìˆëŠ” ì–¸ì–´ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤  
    search_keyword_encode = requests.utils.quote(keyword) 
     
    # ìœ íŠœë¸Œì—ì„œ í•´ë‹¹ í‚¤ì›Œë“œì˜ ì˜ìƒ ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” url ì„ ë§Œë“¤ì–´ì„œ url ë³€ìˆ˜ì— ì…ë ¥í•©ë‹ˆë‹¤ 
    url = "https://www.youtube.com/results?search_query=" + search_keyword_encode 
     
    # í¬ë¡¬ ë“œë¼ì´ë²„ì˜ ìœ„ì¹˜ë¥¼ ì§€ì •í•œ í›„ì— driver ë¼ëŠ” ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ 
    driver = wd.Chrome(executable_path="C:\\data\\chromedriver_win32\\chromedriver.exe") 
     
    # í¬ë¡¬ ë“œë¼ì´ë²„ ê°ì²´ê°€ url ì„ ì½ì–´ë“¤ì…ë‹ˆë‹¤ 
    driver.get(url) 
     
    # í‚¤ì›Œë“œë¡œ ë°›ì•„ì˜¨ ì˜ìƒ ë¦¬ìŠ¤íŠ¸ ì›¹í˜ì´ì§€ì˜ ì²˜ìŒë¶€í„° ë§¨ ë§ˆì§€ë§‰ê¹Œì§€ì˜ ë†’ì´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ 
    last_page_height = driver.execute_script("return document.documentElement.scrollHeight") 

    while True:  # ì•„ë˜ì˜ ì‹¤í–‰ë¬¸ì´ ë¬´í•œíˆ ë°˜ë³µë  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤ 

        # ë§ˆìš°ìŠ¤ ìŠ¤í¬ë¡¤ì„ ëê¹Œì§€ ë‚´ë¦¬ê² ê¸ˆ í•©ë‹ˆë‹¤ 
        driver.execute_script("window.scrollTo(0, document.documentElement.scrollHeight);") 
         
        time.sleep(3.0) # ì ê¹ 3ì´ˆ ì‰½ë‹ˆë‹¤ 
         
        # ë‚´ë¦° ìŠ¤í¬ë¡¤ê¹Œì§€ì˜ ë†’ì´ë¥¼ êµ¬í•´ì„œ  
        new_page_height = driver.execute_script("return document.documentElement.scrollHeight") 
         
        if new_page_height == last_page_height:  # ì§€ê¸ˆì˜ ë†’ì´ê°€ ì›¹í˜ì´ì§€ì˜ ëì´ë¼ë©´ 
            break          # ì¢…ë£Œí•´ë¼ ~ 
             
        last_page_height = new_page_height  # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê³„ì† ìŠ¤í¬ë¡¤ì„ ì§„í–‰í•˜ê²Œ í•©ë‹ˆë‹¤ 
         
    html_source = driver.page_source  # í˜ì´ì§€ì˜ html ì†ŒìŠ¤ë¥¼ html_source ì— ë‹´ìŠµë‹ˆë‹¤ 
     
    driver.quit()   # ë¸Œë¼ìš°ì ¸ ì°½ì„ ë‹«ìŠµë‹ˆë‹¤ 
     
    soup = BeautifulSoup(html_source, 'lxml')   #  ë°›ì•„ì˜¨ html ì½”ë“œë¥¼ beautifulsoup ìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤ 
     
    datas = soup.select("a#video-title")  # a í…Œê·¸ì˜ id ê°€ video-title ì¸ê³³ì˜ html ë¬¸ì„œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ 

    for data in datas: 
        title = data.text.replace('\n', '')   # ë¶ˆëŸ¬ì˜¨ html ì½”ë“œì—ì„œ ì—”í„°ë¥¼ ì—†ì—¡ë‹ˆë‹¤ 
        url = "https://www.youtube.com/" + data.get('href')  # ìƒì„¸ url ì„ ê°€ì ¸ì˜µë‹ˆë‹¤ 
         
        titles.append(title)     #  ì˜ìƒì œëª©ì„ titles ì— append ì‹œí‚µë‹ˆë‹¤ 
        urls.append(url)        #  ìƒì„¸ url ì„ append ì‹œí‚µë‹ˆë‹¤ 
         
    return titles, urls 
```


```
# ë‘ë²ˆì§¸ í•¨ìˆ˜: ëê¹Œì§€ ìŠ¤í¬ë¡¤ë§í•˜ê³ , n ê°œ ì˜ìƒ ëŒ“ê¸€ì— ëŒ€í•œ ìƒì„¸ ì£¼ì†Œ
def crawl_youtube_page_html_sources(urls): 
    html_sources = [] 

    for i in range(0, 2): 
        driver = wd.Chrome(executable_path="C:\\data\\chromedriver_win32\\chromedriver.exe") 
        driver.get(urls[i]) 

        last_page_height = driver.execute_script("return document.documentElement.scrollHeight") 

        while True: 
            driver.execute_script("window.scrollTo(0, document.documentElement.scrollHeight);") 
            time.sleep(3.0) 
            new_page_height = driver.execute_script("return document.documentElement.scrollHeight") 

            if new_page_height == last_page_height: 
                break 
            last_page_height = new_page_height
            
            driver.execute_script('window.scrollTo(0,500)')
            time.sleep(2)

        #í˜ì´ì§€ì˜ html ì†ŒìŠ¤ë¥¼ html_source ì— ë‹´ëŠ”ë‹¤    
        html_source = driver.page_source 
        html_sources.append(html_source) 
        print("OKğŸ’˜") 

        driver.quit() 
    return html_sources 
```


```
# ì„¸ë²ˆì§¸ í•¨ìˆ˜:  ê¸€ì“´ì´, ëŒ“ê¸€ ë¶ˆëŸ¬ì˜¤ê¸°
def get_user_IDs_and_comments(html_sources): 

    import re 
    my_dataframes = []  # ëƒê¸€ ë°ì´í„°ë¥¼ ì €ì¥í•  íŒë‹¤ìŠ¤ ë°ì´í„° í”„ë ˆì„ëª…ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„± 

    #for html in html_sources: # html_sources ë¦¬ìŠ¤íŠ¸ ì•ˆì— ìˆëŠ” html ì½”ë“œë“¤ì„ ì°¨ë¡€ë°ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤ 
    cnt = 0 
    
    while True:      
        html = html_sources[cnt] 
        cnt += 1 
         
        soup = BeautifulSoup(html, 'lxml')  #  lxml ë¡œ ì•ˆë˜ë©´ html.parser ë¼ê³  í•˜ë©´ ë©ë‹ˆë‹¤ 
         
        youtube_user_IDs = soup.select('div#header-author > h3 > a > span')  # ê¸€ì“´ì´ 
        #youtube_user_IDs = soup.select('span.style-scope.ytd-comment-renderer')  
        youtube_comments = soup.select('yt-formatted-string#content-text') # ëƒê¸€ 
        
        #ê¸€ì“´ì´ ë¦¬ìŠ¤íŠ¸ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
        str_youtube_userIDs = [] 
        for i in range(len(youtube_user_IDs)): 
            str_tmp = str(youtube_user_IDs[i].text) 
            str_youtube_userIDs.append(re.sub('[\n\r\t]','',str_tmp)) 
        
        #ëŒ“ê¸€ ì €ì¥ ë¦¬ìŠ¤íŠ¸    
        str_youtube_comments = []     
        for i in range(len(youtube_comments)): 
            str_tmp = str(youtube_comments[i].text) 
            str_youtube_comments.append(re.sub('[\n\r\t]','',str_tmp))             

            
        #ê¸€ì“´ì´ ë¦¬ìŠ¤íŠ¸ì™€ ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸ë¡œ ë”•ì…˜ì–´ë¦¬ë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤  
        # id,comment--->ì»¬ëŸ¼ /  ì•ˆì— ë“¤ì–´ìˆëŠ” ë‚´ìš©ë“¤--->ë°ì´í„°
        pd_data = {"ID":str_youtube_userIDs, "Comment":str_youtube_comments} 

        youtube_pd = pd.DataFrame(pd_data) 

        my_dataframes.append(youtube_pd) 
         
        print('okğŸ˜ˆ') 
        if  cnt ==2: 
            break 
         
    return my_dataframes 
```

ë„¤ë²ˆì§¸ í•¨ìˆ˜ : ì €ì¥ 


```
def convert_csv_from_dataframe(titles, my_dataframes): 
    for i in range(len(my_dataframes)): 
        title = re.sub('[-=+,#/\?:^$.@*\"â€»~&%ã†!ã€\\â€˜|\(\)\[\]\<\>`\'â€¦ã€Š\ã€‹]', '', titles[i]) 
        my_dataframes[i].to_csv("C:\\data\\{}.csv".format(title)) 
```

2. 4ê°œì˜ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ëŠ” ì½”ë“œë¥¼ ê°€ì§€ê³  í•¨ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.  


```
def  youtube_scroll(keyword): 
    title, urls = get_urls_from_youtube_with_keyword(keyword) 
    html_sources =crawl_youtube_page_html_sources(urls) 
    my_dataframes = get_user_IDs_and_comments(html_sources) 
    convert_csv_from_dataframe(title, my_dataframes)   
# êµ¬ê¸€ ì´ë¯¸ì§€ ìŠ¤í¬ë¡¤
def  google_image(keyword):
    ######################################################

    import urllib.request            # íŒŒì´ì¬ì—ì„œ ì›¹ì˜ url ì„ ì¸ì‹í•  ìˆ˜ ìˆê²Œí•˜ëŠ” ëª¨ë“ˆ
    from  bs4 import BeautifulSoup
    from selenium import webdriver  # í¬ë¡¬ ì›¹ë¸Œë¼ìš°ì ¸ë¥¼ ìë™ìœ¼ë¡œ ì œì–´í•˜ê¸° ìœ„í•´ 
    from selenium.webdriver.common.keys import Keys  # í‚¤ë³´ë“œë¥¼ ì»´í“¨í„°ê°€ ì•Œì•„ì„œ ëˆ„ë¥´ê¸° ìœ„í•´ì„œ
    import time                     # ì¤‘ê°„ì¤‘ê°„ sleep ì„ ê±¸ì–´ì•¼ í•´ì„œ time ëª¨ë“ˆ import
    
    
    binary = "C:\\data\\chromedriver_win32\\chromedriver.exe"

    # ë¸Œë¼ìš°ì ¸ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”
    browser = webdriver.Chrome(binary)

    # êµ¬ê¸€ì˜ ì´ë¯¸ì§€ ê²€ìƒ‰ url ë°›ì•„ì˜´(í‚¤ì›Œë“œë¥¼ ì•„ë¬´ê²ƒë„ ì•ˆ ì³¤ì„ë•Œì˜ url)
    browser.get("https://www.google.co.kr/imghp?hl=ko&tab=wi&ei=l1AdWbegOcra8QXvtr-4Cw&ved=0EKouCBUoAQ")

    # ê²€ìƒ‰ì°½ì— ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ë„£ê¸° ìœ„í•´ì„œ ì›¹í˜ì´ì§€ì˜ ê²€ìƒ‰ì°½ì˜ í´ë˜ìŠ¤ ì´ë¦„ì„ ì°¾ì•„ì„œ ê²€ìƒ‰ì°½ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì´ 
    # ì–´ë””ë‹¤ë¼ê³  ì•Œë ¤ì£¼ëŠ” elem ê°ì²´ë¥¼ ë§Œë“­ë‹ˆë‹¤.

    elem = browser.find_element_by_xpath("//*[@class='gLFyf gsfi']") 

    ########################### ê²€ìƒ‰ì–´ ì…ë ¥ ###########################

    # elem ì´ input ì°½ê³¼ ì—°ê²°ë˜ì–´ ìŠ¤ìŠ¤ë¡œ í–„ë²„ê±°ë¥¼ ê²€ìƒ‰
    elem.send_keys(keyword)

    # ì›¹ì—ì„œì˜ submit ì€ ì—”í„°ì˜ ì—­í• ì„ í•¨
    elem.submit()

    ########################### ë°˜ë³µí•  íšŸìˆ˜ ###########################

    # ìŠ¤í¬ë¡¤ì„ ë‚´ë¦¬ë ¤ë©´ ë¸Œë¼ìš°ì ¸ ì´ë¯¸ì§€ ê²€ìƒ‰ê²°ê³¼ ë¶€ë¶„(ë°”ë””ë¶€ë¶„)ì— ë§ˆìš°ìŠ¤ í´ë¦­ í•œë²ˆ í•˜ê³  Endí‚¤ë¥¼ ëˆŒëŸ¬ì•¼í•¨
    for i in range(1, 10):
        browser.find_element_by_xpath("//body").send_keys(Keys.END)
        time.sleep(10)                  # END í‚¤ ëˆ„ë¥´ê³  ë‚´ë ¤ê°€ëŠ”ë° ì‹œê°„ì´ ê±¸ë ¤ì„œ sleep í•´ì¤Œ

    browser.find_element_by_xpath("//*[@class='mye4qd']").send_keys(Keys.ENTER)  # ë”ë³´ê¸° ëˆ„ë¦„

    for i in range(1, 5):
        browser.find_element_by_xpath("//body").send_keys(Keys.END)
        time.sleep(10)

    time.sleep(10)                      # ë„¤íŠ¸ì›Œí¬ ëŠë¦´ê¹Œë´ ì•ˆì •ì„± ìœ„í•´ sleep í•´ì¤Œ
    html = browser.page_source         # í¬ë¡¬ë¸Œë¼ìš°ì ¸ì—ì„œ í˜„ì¬ ë¶ˆëŸ¬ì˜¨ ì†ŒìŠ¤ ê°€ì ¸ì˜´
    soup = BeautifulSoup(html, "lxml") # html ì½”ë“œë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •


    ########################### ê·¸ë¦¼íŒŒì¼ ì €ì¥ ###########################

    def fetch_list_url():  # ì´ë¯¸ì§€ì— ëŒ€í•œ ìƒì„¸ url ì„ params ë¦¬ìŠ¤íŠ¸ì— ë‹´ëŠ” í•¨ìˆ˜
        params = []      
        imgList = soup.find_all("img", class_="rg_i Q4LuWd")  # êµ¬ê¸€ ì´ë¯¸ì§€ url ì´ ìˆëŠ” img íƒœê·¸ì˜ _img í´ë˜ìŠ¤ì— ê°€ì„œ
        for im in imgList:
            try :
                params.append(im["src"])                   # params ë¦¬ìŠ¤íŠ¸ì— image url ì„ ë‹´ìŒ
            except KeyError:
                params.append(im["data-src"])  # src ì—ì„œ ëª»ê°€ì ¸ì˜¤ë©´ data-src ì—ì„œ ê°€ì ¸ì™€ë¼
        return params

    def   fetch_detail_url():     # ìƒì„¸ url ì˜ ì´ë¯¸ì§€ë“¤ì„ ìš°ë¦¬ ì»´í“¨í„°ì— ë‹¤ìš´ë¡œë“œ ë°›ëŠ” í•¨ìˆ˜
        params = fetch_list_url()  # fetch_list_url() í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ê³  params ë¦¬ìŠ¤íŠ¸ì—  ìƒì„¸ url ì„ ë„£ìŠµë‹ˆë‹¤.

        for idx, p in enumerate(params,1):                      
            urllib.request.urlretrieve( p , "/Users/munhuijung/googleimage/" + str(idx) + ".jpg")

    # enumerate ëŠ” ë¦¬ìŠ¤íŠ¸ì˜ ëª¨ë“  ìš”ì†Œë¥¼ ì¸ë±ìŠ¤ì™€ ìŒìœ¼ë¡œ ì¶”ì¶œ
    # í•˜ëŠ” í•¨ìˆ˜ . ìˆ«ì 1ì€ ì¸ë±ìŠ¤ë¥¼ 1ë¶€í„° ì‹œì‘í•´ë¼ ~

    fetch_detail_url()

    # ëë‚˜ë©´ ë¸Œë¼ìš°ì ¸ ë‹«ê¸°
    browser.quit()
```


```
# bing ì´ë¯¸ì§€ ê²€ìƒ‰
def bing_image(keyword):

    ######################################################

    import urllib.request            # íŒŒì´ì¬ì—ì„œ ì›¹ì˜ url ì„ ì¸ì‹í•  ìˆ˜ ìˆê²Œí•˜ëŠ” ëª¨ë“ˆ
    from  bs4 import BeautifulSoup
    from selenium import webdriver  # í¬ë¡¬ ì›¹ë¸Œë¼ìš°ì ¸ë¥¼ ìë™ìœ¼ë¡œ ì œì–´í•˜ê¸° ìœ„í•´ 
    from selenium.webdriver.common.keys import Keys  # í‚¤ë³´ë“œë¥¼ ì»´í“¨í„°ê°€ ì•Œì•„ì„œ ëˆ„ë¥´ê¸° ìœ„í•´ì„œ
    import time                     # ì¤‘ê°„ì¤‘ê°„ sleep ì„ ê±¸ì–´ì•¼ í•´ì„œ time ëª¨ë“ˆ import

    ########################### url ë°›ì•„ì˜¤ê¸° ###########################

    # ì›¹ë¸Œë¼ìš°ì ¸ë¡œ í¬ë¡¬ì„ ì‚¬ìš©í• ê±°ë¼ì„œ í¬ë¡¬ ë“œë¼ì´ë²„ë¥¼ ë‹¤ìš´ë°›ì•„ ìœ„ì˜ ìœ„ì¹˜ì— ë‘”ë‹¤
    # íŒ¬í…€ jsë¡œ í•˜ë©´ ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŒ

    binary = "C:\\data\\chromedriver_win32\\chromedriver.exe"

    # ë¸Œë¼ìš°ì ¸ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”
    browser = webdriver.Chrome(binary)

    # ë¹™ ì´ë¯¸ì§€ ê²€ìƒ‰ url ë°›ì•„ì˜´(í‚¤ì›Œë“œë¥¼ ì•„ë¬´ê²ƒë„ ì•ˆ ì³¤ì„ë•Œì˜ url)
    browser.get("https://www.bing.com/?scope=images&nr=1&FORM=NOFORM")
    time.sleep(5)

    # ê²€ìƒ‰ì°½ì— ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ë„£ê¸° ìœ„í•´ì„œ ì›¹í˜ì´ì§€ì˜ ê²€ìƒ‰ì°½ì˜ í´ë˜ìŠ¤ ì´ë¦„ì„ ì°¾ì•„ì„œ ê²€ìƒ‰ì°½ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì´ 
    # ì–´ë””ë‹¤ë¼ê³  ì•Œë ¤ì£¼ëŠ” elem ê°ì²´ë¥¼ ë§Œë“­ë‹ˆë‹¤.

    elem = browser.find_element_by_id("sb_form_q") 

    ########################### ê²€ìƒ‰ì–´ ì…ë ¥ ###########################

    # elem ì´ input ì°½ê³¼ ì—°ê²°ë˜ì–´ ìŠ¤ìŠ¤ë¡œ í–„ë²„ê±°ë¥¼ ê²€ìƒ‰
    elem.send_keys(keyword)

    # ì›¹ì—ì„œì˜ submit ì€ ì—”í„°ì˜ ì—­í• ì„ í•¨
    elem.submit()

    ########################### ë°˜ë³µí•  íšŸìˆ˜ ###########################

    # ìŠ¤í¬ë¡¤ì„ ë‚´ë¦¬ë ¤ë©´ ë¸Œë¼ìš°ì ¸ ì´ë¯¸ì§€ ê²€ìƒ‰ê²°ê³¼ ë¶€ë¶„(ë°”ë””ë¶€ë¶„)ì— ë§ˆìš°ìŠ¤ í´ë¦­ í•œë²ˆ í•˜ê³  Endí‚¤ë¥¼ ëˆŒëŸ¬ì•¼í•¨
    for i in range(1, 10):
        browser.find_element_by_xpath("//body").send_keys(Keys.END)
        time.sleep(10)                  # END í‚¤ ëˆ„ë¥´ê³  ë‚´ë ¤ê°€ëŠ”ë° ì‹œê°„ì´ ê±¸ë ¤ì„œ sleep í•´ì¤Œ


    time.sleep(10)                      # ë„¤íŠ¸ì›Œí¬ ëŠë¦´ê¹Œë´ ì•ˆì •ì„± ìœ„í•´ sleep í•´ì¤Œ
    html = browser.page_source         # í¬ë¡¬ë¸Œë¼ìš°ì ¸ì—ì„œ í˜„ì¬ ë¶ˆëŸ¬ì˜¨ ì†ŒìŠ¤ ê°€ì ¸ì˜´
    soup = BeautifulSoup(html, "lxml") # html ì½”ë“œë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •


    ########################### ê·¸ë¦¼íŒŒì¼ ì €ì¥ ###########################

    def fetch_list_url():  # ì´ë¯¸ì§€ì— ëŒ€í•œ ìƒì„¸ url ì„ params ë¦¬ìŠ¤íŠ¸ì— ë‹´ëŠ” í•¨ìˆ˜
        params = []      
        imgList = soup.find_all("img", class_="mimg")  # ë¹™ ì´ë¯¸ì§€ url ì´ ìˆëŠ” img íƒœê·¸ì˜ mimg í´ë˜ìŠ¤ì— ê°€ì„œ
        for im in imgList:
            try :
                params.append(im["src"])                   # params ë¦¬ìŠ¤íŠ¸ì— image url ì„ ë‹´ìŒ
            except KeyError:
                params.append(im["data-src"])  # src ì—ì„œ ëª»ê°€ì ¸ì˜¤ë©´ data-src ì—ì„œ ê°€ì ¸ì™€ë¼
        return params

    def   fetch_detail_url():     # ìƒì„¸ url ì˜ ì´ë¯¸ì§€ë“¤ì„ ìš°ë¦¬ ì»´í“¨í„°ì— ë‹¤ìš´ë¡œë“œ ë°›ëŠ” í•¨ìˆ˜
        params = fetch_list_url()  # fetch_list_url() í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ê³  params ë¦¬ìŠ¤íŠ¸ì—  ìƒì„¸ url ì„ ë„£ìŠµë‹ˆë‹¤.

        for   idx ,  p  in enumerate(params,1):  #  params ë¦¬ìŠ¤íŠ¸ì˜ ìš”ì†Œë¥¼ í•˜ë‚˜ì”© ê°€ì ¸ì˜¤ëŠ”ë° ë²ˆí˜¸ë¥¼ ë¶€ì—¬í•´ì„œ ê°€ì ¸
            # ë‹¤ìš´ë°›ì„ í´ë”ê²½ë¡œ ì…ë ¥              # ì˜µë‹ˆë‹¤. ê·¸ëŸ°ë° 1ë²ˆë¶€í„° ë¶€ì—¬í•©ë‹ˆë‹¤.
            urllib.request.urlretrieve( p ,"C:\data\bing" + str(idx) + ".jpg")

    # enumerate ëŠ” ë¦¬ìŠ¤íŠ¸ì˜ ëª¨ë“  ìš”ì†Œë¥¼ ì¸ë±ìŠ¤ì™€ ìŒìœ¼ë¡œ ì¶”ì¶œ
    # í•˜ëŠ” í•¨ìˆ˜ . ìˆ«ì 1ì€ ì¸ë±ìŠ¤ë¥¼ 1ë¶€í„° ì‹œì‘í•´ë¼ ~

    fetch_detail_url()

    # ëë‚˜ë©´ ë¸Œë¼ìš°ì ¸ ë‹«ê¸°
    browser.quit()
def mysql_connect(table_name):
    import  pymysql
    import  pandas  as  pd 

    conn = pymysql.connect( host = "localhost", user="root", password="ansgml1201.",
                                      db ="orcl", charset ="utf8")
    cursors = conn.cursor()  # mySQL ì— ì ‘ì†í•œìˆ˜ ìƒì„±ëœ ë©”ëª¨ë¦¬ ì´ë¦„ì„ cursors ë¡œ í•˜ê² ë‹¤
    sql = "select * from " + table_name
    cursors.execute(sql)    #  sql ë¬¸ì¥ì„ ì‹¤í–‰í•´ì„œ ê²°ê³¼ë¥¼ cursors ë¼ëŠ” ë©”ëª¨ë¦¬ì— ë‹´ìŠµë‹ˆë‹¤
    rows = cursors.fetchall()   # ë©”ëª¨ë¦¬ì— ìˆëŠ” ê²°ê³¼ë¥¼ rows ì— ë„£ìŠµë‹ˆë‹¤

    colname = cursors.description     #emp í…Œì´ë¸”ì˜ ì»¬ëŸ¼ì´ë¦„ì´ í¬í•¨ëœ ì •ë³´ë¥¼ colname ì— ë‹´ìŒ
    col = []
    for  i  in  colname:
        col.append( i[0].lower() )  # emp í…Œì´ë¸”ì˜ ì»¬ëŸ¼ì´ë¦„ì„ ì†Œë¬¸ìë¡œ ë³€ê²½í•´ì„œ col ë¦¬ìŠ¤íŠ¸ì— ë‹´ìŒ

    dataframe = pd.DataFrame( rows, columns = col )
    return  dataframe
```


```
result = None   # ì „ì—­ë³€ìˆ˜ë¥¼ ì„ ì–¸í•©ë‹ˆë‹¤.
```


```
def Oracle_connect(table_name):
    table_name = input('resultì— ë‹´ê³  ì‹¶ì€ í…Œì´ë¸”ëª…')
    import cx_Oracle
    import pandas as pd

    dsn = cx_Oracle.makedsn("localhost",1521,"xe")
    db = cx_Oracle.connect("c##scott","tiger","dsn")
    coursor = db.cursor()
    cursor.excute('select*from'+table_name)
    rows = cursor2.fetchall()
    emp = pd.DataFrame(rows)
    colname = cursor.description
    col = []
    for i in colname:
        col.append(i[0].lower())
        
    result2 = pd.DataFrame(rows, columns = col)
    return result2 
```


```
result2=None
```
